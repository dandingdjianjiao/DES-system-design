#!/usr/bin/env python3
"""
æµ‹è¯•LargeRAGå·¥å…·å¯¹ä¸åŒLLMæœåŠ¡çš„å…¼å®¹æ€§
"""

import os
import sys
import yaml
from pathlib import Path
from dotenv import load_dotenv

def load_settings():
    """åŠ è½½settings.yamlé…ç½®æ–‡ä»¶"""
    settings_path = Path(__file__).parent / "src" / "config" / "settings.yaml"
    try:
        with open(settings_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
        return None

def test_openai_compatible_service():
    """æµ‹è¯•OpenAIå…¼å®¹æœåŠ¡é…ç½®"""
    print("ğŸ” æµ‹è¯•OpenAIå…¼å®¹æœåŠ¡é…ç½®...")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    settings = load_settings()
    if not settings:
        return False
    
    # ä»é…ç½®æ–‡ä»¶è·å–æ¨¡å‹ä¿¡æ¯
    llm_config = settings.get('llm', {})
    embedding_config = settings.get('embedding', {})
    
    llm_model = llm_config.get('model', 'gpt-3.5-turbo')
    embedding_model = embedding_config.get('model', 'text-embedding-ada-002')
    
    # è·å–APIé…ç½®
    api_key_env = llm_config.get('api_key_env', 'OPENAI_API_KEY')
    api_base_env = llm_config.get('api_base_env', 'OPENAI_API_BASE')
    
    api_key = os.getenv(api_key_env)
    api_base = os.getenv(api_base_env)
    
    if not api_key:
        print(f"âš ï¸  {api_key_env}æœªè®¾ç½®ï¼Œè·³è¿‡æµ‹è¯•")
        return True
    
    print(f"ğŸ“‹ é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è®¾ç½®:")
    print(f"   LLMæ¨¡å‹: {llm_model}")
    print(f"   åµŒå…¥æ¨¡å‹: {embedding_model}")
    print(f"   æ¸©åº¦: {llm_config.get('temperature', 0.1)}")
    print(f"   æœ€å¤§ä»¤ç‰Œ: {llm_config.get('max_tokens', 4000)}")
    
    try:
        from llama_index.llms.openai import OpenAI
        from llama_index.embeddings.openai import OpenAIEmbedding
        
        # é…ç½®å‚æ•°ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
        llm_kwargs = {
            "api_key": api_key,
            "temperature": llm_config.get('temperature', 0.1),
            "max_tokens": llm_config.get('max_tokens', 4000),
        }
        
        embedding_kwargs = {
            "api_key": api_key,
        }
        
        # å¦‚æœè®¾ç½®äº†è‡ªå®šä¹‰APIåŸºç¡€URL
        if api_base:
            llm_kwargs["api_base"] = api_base
            embedding_kwargs["api_base"] = api_base
            print(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰APIåŸºç¡€URL: {api_base}")
        else:
            print("âœ… ä½¿ç”¨é»˜è®¤OpenAI API")
        
        # æ ¹æ®APIåŸºç¡€URLæ¨æ–­æœåŠ¡ç±»å‹
        service_info = detect_service_type(api_base)
        print(f"âœ… æ£€æµ‹åˆ°æœåŠ¡ç±»å‹: {service_info['name']}")
        
        # åˆ›å»ºLLMå®ä¾‹ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹ï¼‰
        try:
            llm = OpenAI(
                model=llm_model,
                **llm_kwargs
            )
            print(f"âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ: {llm.model}")
        except Exception as e:
            print(f"âš ï¸  LLMå®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
            print("   å¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹åç§°æˆ–ä½¿ç”¨ä¸“ç”¨çš„LLMé›†æˆåŒ…")
        
        # åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
        try:
            # æ ¹æ®APIåŸºç¡€URLé€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹ç±»
            if api_base and "dashscope.aliyuncs.com" in api_base:
                # ä½¿ç”¨DashScopeä¸“ç”¨çš„åµŒå…¥æ¨¡å‹ç±»
                try:
                    from llama_index.embeddings.dashscope import DashScopeEmbedding
                    
                    # è®¾ç½®DashScope APIå¯†é’¥
                    os.environ["DASHSCOPE_API_KEY"] = api_key
                    
                    embedding = DashScopeEmbedding(
                        model_name=embedding_model,
                        api_key=api_key
                    )
                    print(f"âœ… DashScopeåµŒå…¥æ¨¡å‹å®ä¾‹åˆ›å»ºæˆåŠŸ: {embedding_model}")
                    
                except ImportError:
                    print("âš ï¸  DashScopeåµŒå…¥æ¨¡å‹åŒ…æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼")
                    embedding = OpenAIEmbedding(
                        model=embedding_model,
                        **embedding_kwargs
                    )
                    print(f"âœ… åµŒå…¥æ¨¡å‹å®ä¾‹åˆ›å»ºæˆåŠŸ: {embedding.model_name}")
            else:
                # ä½¿ç”¨æ ‡å‡†OpenAIåµŒå…¥æ¨¡å‹
                embedding = OpenAIEmbedding(
                    model=embedding_model,
                    **embedding_kwargs
                )
                print(f"âœ… åµŒå…¥æ¨¡å‹å®ä¾‹åˆ›å»ºæˆåŠŸ: {embedding.model_name}")
                
        except Exception as e:
            print(f"âš ï¸  åµŒå…¥æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            print("   å»ºè®®: ä½¿ç”¨DashScopeä¸“ç”¨åµŒå…¥åŒ…æˆ–è°ƒæ•´æ¨¡å‹åç§°")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

def detect_service_type(api_base):
    """æ ¹æ®APIåŸºç¡€URLæ£€æµ‹æœåŠ¡ç±»å‹"""
    if not api_base:
        return {
            "name": "OpenAIå®˜æ–¹",
            "llm_models": ["gpt-4", "gpt-3.5-turbo"],
            "embedding_models": ["text-embedding-ada-002", "text-embedding-3-small"]
        }
    
    api_base = api_base.lower()
    
    if "dashscope.aliyuncs.com" in api_base:
        return {
            "name": "é€šä¹‰åƒé—® (Qwen)",
            "llm_models": ["qwen-turbo", "qwen-plus", "qwen-max"],
            "embedding_models": ["text-embedding-v1"]
        }
    elif "bigmodel.cn" in api_base:
        return {
            "name": "æ™ºè°±AI (GLM)",
            "llm_models": ["glm-4", "glm-3-turbo"],
            "embedding_models": ["embedding-2"]
        }
    elif "moonshot.cn" in api_base:
        return {
            "name": "æœˆä¹‹æš—é¢ (Kimi)",
            "llm_models": ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"],
            "embedding_models": []  # Kimiä¸»è¦æä¾›å¯¹è¯æ¨¡å‹
        }
    elif "localhost" in api_base or "127.0.0.1" in api_base:
        return {
            "name": "æœ¬åœ°éƒ¨ç½²æœåŠ¡",
            "llm_models": ["local-model"],  # éœ€è¦æ ¹æ®å®é™…éƒ¨ç½²çš„æ¨¡å‹è°ƒæ•´
            "embedding_models": ["local-embedding"]
        }
    else:
        return {
            "name": "æœªçŸ¥å…¼å®¹æœåŠ¡",
            "llm_models": ["gpt-3.5-turbo"],  # ä½¿ç”¨é€šç”¨æ¨¡å‹å
            "embedding_models": ["text-embedding-ada-002"]
        }

def show_configuration_examples():
    """æ˜¾ç¤ºä¸åŒæœåŠ¡çš„é…ç½®ç¤ºä¾‹"""
    print("\nğŸ“‹ ä¸åŒæœåŠ¡é…ç½®ç¤ºä¾‹:")
    print("=" * 60)
    
    examples = [
        {
            "name": "é€šä¹‰åƒé—® (Qwen)",
            "api_key": "your_dashscope_api_key",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "llm_model": "qwen-turbo",
            "embedding_model": "text-embedding-v1",
            "notes": "éœ€è¦åœ¨é˜¿é‡Œäº‘æ§åˆ¶å°è·å–APIå¯†é’¥"
        },
        {
            "name": "æ™ºè°±AI (GLM)",
            "api_key": "your_zhipuai_api_key",
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "llm_model": "glm-4",
            "embedding_model": "embedding-2",
            "notes": "éœ€è¦åœ¨æ™ºè°±AIå¼€æ”¾å¹³å°æ³¨å†Œè·å–å¯†é’¥"
        },
        {
            "name": "æœˆä¹‹æš—é¢ (Kimi)",
            "api_key": "your_moonshot_api_key",
            "api_base": "https://api.moonshot.cn/v1",
            "llm_model": "moonshot-v1-8k",
            "embedding_model": "éœ€è¦å…¶ä»–æœåŠ¡",
            "notes": "ä¸»è¦æä¾›å¯¹è¯æ¨¡å‹ï¼ŒåµŒå…¥éœ€è¦é…åˆå…¶ä»–æœåŠ¡"
        }
    ]
    
    for example in examples:
        print(f"\nğŸ”§ {example['name']}:")
        print(f"   APIå¯†é’¥: {example['api_key']}")
        print(f"   APIåŸºç¡€URL: {example['api_base']}")
        print(f"   æ¨èLLMæ¨¡å‹: {example['llm_model']}")
        print(f"   æ¨èåµŒå…¥æ¨¡å‹: {example['embedding_model']}")
        print(f"   æ³¨æ„äº‹é¡¹: {example['notes']}")

def test_model_functionality():
    """æµ‹è¯•æ¨¡å‹çš„å®é™…åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹å®é™…åŠŸèƒ½...")
    
    # åŠ è½½ç¯å¢ƒå˜é‡å’Œé…ç½®
    load_dotenv()
    settings = load_settings()
    if not settings:
        return False
    
    llm_config = settings.get('llm', {})
    api_key = os.getenv(llm_config.get('api_key_env', 'OPENAI_API_KEY'))
    api_base = os.getenv(llm_config.get('api_base_env', 'OPENAI_API_BASE'))
    
    if not api_key:
        print("âš ï¸  APIå¯†é’¥æœªè®¾ç½®ï¼Œè·³è¿‡åŠŸèƒ½æµ‹è¯•")
        return True
    
    try:
        # æ ¹æ®APIåŸºç¡€URLé€‰æ‹©åˆé€‚çš„LLMç±»
        if api_base and "dashscope.aliyuncs.com" in api_base:
            # ä½¿ç”¨DashScopeä¸“ç”¨çš„LLMç±»
            try:
                from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels
                
                # è®¾ç½®DashScope APIå¯†é’¥
                os.environ["DASHSCOPE_API_KEY"] = api_key
                
                # åˆ›å»ºDashScope LLMå®ä¾‹
                llm = DashScope(
                    model_name=llm_config.get('model', 'qwen-turbo'),
                    api_key=api_key,
                    temperature=llm_config.get('temperature', 0.1),
                    max_tokens=500
                )
                
                print("âœ… ä½¿ç”¨DashScopeä¸“ç”¨LLMç±»")
                
            except ImportError:
                print("âš ï¸  DashScope LLMç±»æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼")
                # å›é€€åˆ°OpenAIå…¼å®¹æ¨¡å¼
                from llama_index.llms.openai import OpenAI
                
                llm = OpenAI(
                    model=llm_config.get('model', 'qwen-turbo'),
                    api_key=api_key,
                    api_base=api_base,
                    temperature=llm_config.get('temperature', 0.1),
                    max_tokens=500
                )
        else:
            # ä½¿ç”¨æ ‡å‡†OpenAI LLMç±»
            from llama_index.llms.openai import OpenAI
            
            llm_kwargs = {
                "api_key": api_key,
                "temperature": llm_config.get('temperature', 0.1),
                "max_tokens": 500,
            }
            
            if api_base:
                llm_kwargs["api_base"] = api_base
            
            llm = OpenAI(
                model=llm_config.get('model', 'gpt-3.5-turbo'),
                **llm_kwargs
            )
        
        # æµ‹è¯•ç®€å•çš„æ–‡æœ¬ç”Ÿæˆ
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æ·±å…±ç†”æº¶å‰‚(Deep Eutectic Solvent)çš„åŸºæœ¬æ¦‚å¿µã€‚"
        print(f"ğŸ“ æµ‹è¯•æç¤º: {test_prompt}")
        
        response = llm.complete(test_prompt)
        print(f"âœ… æ¨¡å‹å“åº”æˆåŠŸ:")
        print(f"   å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
        print(f"   å“åº”é¢„è§ˆ: {response.text[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        print(f"   å»ºè®®: æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€ƒè™‘å®‰è£…å¯¹åº”çš„LLMé›†æˆåŒ…")
        return False

def show_current_config():
    """æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯"""
    print("\nğŸ“‹ å½“å‰é…ç½®ä¿¡æ¯:")
    print("=" * 60)
    
    settings = load_settings()
    if not settings:
        return
    
    llm_config = settings.get('llm', {})
    embedding_config = settings.get('embedding', {})
    
    print(f"LLMé…ç½®:")
    print(f"  æ¨¡å‹: {llm_config.get('model', 'æœªè®¾ç½®')}")
    print(f"  APIå¯†é’¥ç¯å¢ƒå˜é‡: {llm_config.get('api_key_env', 'æœªè®¾ç½®')}")
    print(f"  APIåŸºç¡€URLç¯å¢ƒå˜é‡: {llm_config.get('api_base_env', 'æœªè®¾ç½®')}")
    print(f"  æ¸©åº¦: {llm_config.get('temperature', 'æœªè®¾ç½®')}")
    print(f"  æœ€å¤§ä»¤ç‰Œ: {llm_config.get('max_tokens', 'æœªè®¾ç½®')}")
    
    print(f"\nåµŒå…¥æ¨¡å‹é…ç½®:")
    print(f"  æ¨¡å‹: {embedding_config.get('model', 'æœªè®¾ç½®')}")
    print(f"  APIå¯†é’¥ç¯å¢ƒå˜é‡: {embedding_config.get('api_key_env', 'æœªè®¾ç½®')}")
    print(f"  APIåŸºç¡€URLç¯å¢ƒå˜é‡: {embedding_config.get('api_base_env', 'æœªè®¾ç½®')}")
    print(f"  æ‰¹å¤„ç†å¤§å°: {embedding_config.get('batch_size', 'æœªè®¾ç½®')}")

def show_installation_suggestions():
    """æ˜¾ç¤ºå®‰è£…å»ºè®®"""
    print("\nï¿½ æ¨èå®‰è£…çš„Lç¤ºLMé›†æˆåŒ…:")
    print("=" * 60)
    
    suggestions = [
        {
            "service": "é€šä¹‰åƒé—® (DashScope)",
            "packages": [
                "pip install llama-index-llms-dashscope",
                "pip install llama-index-embeddings-dashscope"
            ],
            "env_vars": ["DASHSCOPE_API_KEY"],
            "notes": "ä¸“ç”¨é›†æˆåŒ…ï¼Œæ€§èƒ½æ›´å¥½ï¼ŒåŠŸèƒ½æ›´å®Œæ•´"
        },
        {
            "service": "æ™ºè°±AI (GLM)",
            "packages": [
                "pip install llama-index-llms-zhipuai"
            ],
            "env_vars": ["ZHIPUAI_API_KEY"],
            "notes": "ä¸“ç”¨é›†æˆåŒ…ï¼Œæ”¯æŒGLMç³»åˆ—æ¨¡å‹"
        },
        {
            "service": "OpenAIå…¼å®¹æœåŠ¡",
            "packages": [
                "pip install llama-index-llms-openai"
            ],
            "env_vars": ["OPENAI_API_KEY", "OPENAI_API_BASE"],
            "notes": "é€šç”¨å…¼å®¹æ–¹å¼ï¼Œä½†å¯èƒ½æœ‰æ¨¡å‹åç§°é™åˆ¶"
        }
    ]
    
    for suggestion in suggestions:
        print(f"\nğŸ”§ {suggestion['service']}:")
        print("   å®‰è£…å‘½ä»¤:")
        for package in suggestion['packages']:
            print(f"     {package}")
        print(f"   ç¯å¢ƒå˜é‡: {', '.join(suggestion['env_vars'])}")
        print(f"   è¯´æ˜: {suggestion['notes']}")

def show_yaml_config_example():
    """æ˜¾ç¤ºYAMLé…ç½®æ–‡ä»¶ä¿®æ”¹ç¤ºä¾‹"""
    print("\nğŸ“ é…ç½®æ–‡ä»¶ä¿®æ”¹ç¤ºä¾‹:")
    print("=" * 60)
    
    yaml_example = """
# ä½¿ç”¨é€šä¹‰åƒé—®çš„é…ç½®ç¤ºä¾‹
llm:
  model: "qwen-turbo"  # æ”¹ä¸ºqwenæ¨¡å‹
  api_key_env: "OPENAI_API_KEY"
  api_base_env: "OPENAI_API_BASE"
  temperature: 0.1
  max_tokens: 4000

embedding:
  model: "text-embedding-v1"  # æ”¹ä¸ºqwenåµŒå…¥æ¨¡å‹
  api_key_env: "OPENAI_API_KEY"
  api_base_env: "OPENAI_API_BASE"
  batch_size: 100
"""
    
    print(yaml_example)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ” æµ‹è¯•LargeRAGå·¥å…·å¯¹ä¸åŒLLMæœåŠ¡çš„å…¼å®¹æ€§...")
    print("=" * 60)
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    show_current_config()
    
    # æµ‹è¯•é…ç½®å…¼å®¹æ€§
    config_ok = test_openai_compatible_service()
    
    # æµ‹è¯•æ¨¡å‹åŠŸèƒ½ï¼ˆå¦‚æœé…ç½®æ­£ç¡®ï¼‰
    functionality_ok = True
    if config_ok:
        functionality_ok = test_model_functionality()
    
    # æ˜¾ç¤ºé…ç½®ç¤ºä¾‹
    show_configuration_examples()
    show_yaml_config_example()
    show_installation_suggestions()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   é…ç½®å…¼å®¹æ€§: {'âœ…' if config_ok else 'âŒ'}")
    print(f"   æ¨¡å‹åŠŸèƒ½æ€§: {'âœ…' if functionality_ok else 'âŒ'}")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. ä¼˜å…ˆä½¿ç”¨ä¸“ç”¨çš„LLMé›†æˆåŒ…ï¼Œè€Œä¸æ˜¯OpenAIå…¼å®¹æ¨¡å¼")
    print("2. æ ¹æ®éœ€è¦ä¿®æ”¹.envæ–‡ä»¶ä¸­çš„APIå¯†é’¥å’ŒåŸºç¡€URL")
    print("3. åœ¨settings.yamlä¸­è°ƒæ•´modelå‚æ•°ä¸ºå¯¹åº”æœåŠ¡çš„æ¨¡å‹å")
    print("4. æ³¨æ„ä¸åŒæœåŠ¡çš„æ¨¡å‹åç§°å’Œå‚æ•°å¯èƒ½æœ‰å·®å¼‚")
    print("5. æŸäº›æœåŠ¡å¯èƒ½ä¸æ”¯æŒåµŒå…¥æ¨¡å‹ï¼Œéœ€è¦æ··åˆä½¿ç”¨")
    print("6. å¦‚æœæµ‹è¯•å¤±è´¥ï¼Œè€ƒè™‘å®‰è£…å¯¹åº”çš„ä¸“ç”¨é›†æˆåŒ…")
    
    return 0 if (config_ok and functionality_ok) else 1

if __name__ == "__main__":
    sys.exit(main())